nohup: ignoring input
Using device: cuda
[Seed=101] Epoch 1/40 => train_loss=4.4272, val_loss=4.2011
[Seed=101] Epoch 2/40 => train_loss=4.0513, val_loss=3.5017
[Seed=101] Epoch 3/40 => train_loss=1.1437, val_loss=0.2944
[Seed=101] Epoch 4/40 => train_loss=0.3377, val_loss=0.2908
[Seed=101] Epoch 5/40 => train_loss=0.3145, val_loss=0.2894
[Seed=101] Epoch 6/40 => train_loss=0.3035, val_loss=0.2891
[Seed=101] Epoch 7/40 => train_loss=0.3002, val_loss=0.2885
[Seed=101] Epoch 8/40 => train_loss=0.2973, val_loss=0.2888
[Seed=101] Epoch 9/40 => train_loss=0.2954, val_loss=0.2883
[Seed=101] Epoch 10/40 => train_loss=0.2941, val_loss=0.2888
[Seed=101] Epoch 11/40 => train_loss=0.2936, val_loss=0.2883
[Seed=101] Epoch 12/40 => train_loss=0.2926, val_loss=0.2884
[Seed=101] Epoch 13/40 => train_loss=0.2926, val_loss=0.2885
[Seed=101] Epoch 14/40 => train_loss=0.2914, val_loss=0.2882
[Seed=101] Epoch 15/40 => train_loss=0.2921, val_loss=0.2885
[Seed=101] Epoch 16/40 => train_loss=0.2900, val_loss=0.2881
[Seed=101] Epoch 17/40 => train_loss=0.2895, val_loss=0.2882
[Seed=101] Epoch 18/40 => train_loss=0.2909, val_loss=0.2883
[Seed=101] Epoch 19/40 => train_loss=0.2903, val_loss=0.2881
[Seed=101] Epoch 20/40 => train_loss=0.2903, val_loss=0.2882
[Seed=101] Epoch 21/40 => train_loss=0.2893, val_loss=0.2882
[Seed=101] Epoch 22/40 => train_loss=0.2900, val_loss=0.2880
[Seed=101] Epoch 23/40 => train_loss=0.2885, val_loss=0.2882
[Seed=101] Epoch 24/40 => train_loss=0.2898, val_loss=0.2882
[Seed=101] Epoch 25/40 => train_loss=0.2891, val_loss=0.2881
[Seed=101] Epoch 26/40 => train_loss=0.2892, val_loss=0.2881
[Seed=101] Epoch 27/40 => train_loss=0.2889, val_loss=0.2882
[Seed=101] Epoch 28/40 => train_loss=0.2891, val_loss=0.2880
[Seed=101] Epoch 29/40 => train_loss=0.2882, val_loss=0.2881
[Seed=101] Epoch 30/40 => train_loss=0.2889, val_loss=0.2880
[Seed=101] Epoch 31/40 => train_loss=0.2885, val_loss=0.2881
[Seed=101] Epoch 32/40 => train_loss=0.2879, val_loss=0.2880
[Seed=101] Epoch 33/40 => train_loss=0.2886, val_loss=0.2883
[Seed=101] Epoch 34/40 => train_loss=0.2887, val_loss=0.2881
[Seed=101] Epoch 35/40 => train_loss=0.2882, val_loss=0.2880
[Seed=101] Epoch 36/40 => train_loss=0.2880, val_loss=0.2882
[Seed=101] Epoch 37/40 => train_loss=0.2885, val_loss=0.2880
[Seed=101] Epoch 38/40 => train_loss=0.2877, val_loss=0.2880
[Seed=101] Epoch 39/40 => train_loss=0.2885, val_loss=0.2883
[Seed=101] Epoch 40/40 => train_loss=0.2879, val_loss=0.2881
[Seed=23] Epoch 1/40 => train_loss=4.4164, val_loss=4.1632
[Seed=23] Epoch 2/40 => train_loss=4.0029, val_loss=3.1488
[Seed=23] Epoch 3/40 => train_loss=1.1593, val_loss=0.3801
[Seed=23] Epoch 4/40 => train_loss=0.4881, val_loss=0.3141
[Seed=23] Epoch 5/40 => train_loss=0.3707, val_loss=0.2911
[Seed=23] Epoch 6/40 => train_loss=0.3271, val_loss=0.2895
[Seed=23] Epoch 7/40 => train_loss=0.3121, val_loss=0.2887
[Seed=23] Epoch 8/40 => train_loss=0.3047, val_loss=0.2887
[Seed=23] Epoch 9/40 => train_loss=0.3022, val_loss=0.2882
[Seed=23] Epoch 10/40 => train_loss=0.2964, val_loss=0.2885
[Seed=23] Epoch 11/40 => train_loss=0.2964, val_loss=0.2884
[Seed=23] Epoch 12/40 => train_loss=0.2938, val_loss=0.2882
[Seed=23] Epoch 13/40 => train_loss=0.2944, val_loss=0.2883
[Seed=23] Epoch 14/40 => train_loss=0.2918, val_loss=0.2884
[Seed=23] Epoch 15/40 => train_loss=0.2923, val_loss=0.2882
[Seed=23] Epoch 16/40 => train_loss=0.2920, val_loss=0.2880
[Seed=23] Epoch 17/40 => train_loss=0.2903, val_loss=0.2883
[Seed=23] Epoch 18/40 => train_loss=0.2911, val_loss=0.2881
[Seed=23] Epoch 19/40 => train_loss=0.2901, val_loss=0.2881
[Seed=23] Epoch 20/40 => train_loss=0.2897, val_loss=0.2881
[Seed=23] Epoch 21/40 => train_loss=0.2900, val_loss=0.2881
[Seed=23] Epoch 22/40 => train_loss=0.2902, val_loss=0.2881
[Seed=23] Epoch 23/40 => train_loss=0.2893, val_loss=0.2881
[Seed=23] Epoch 24/40 => train_loss=0.2899, val_loss=0.2880
[Seed=23] Epoch 25/40 => train_loss=0.2883, val_loss=0.2881
[Seed=23] Epoch 26/40 => train_loss=0.2891, val_loss=0.2882
[Seed=23] Epoch 27/40 => train_loss=0.2889, val_loss=0.2881
[Seed=23] Epoch 28/40 => train_loss=0.2882, val_loss=0.2881
[Seed=23] Epoch 29/40 => train_loss=0.2892, val_loss=0.2882
[Seed=23] Epoch 30/40 => train_loss=0.2883, val_loss=0.2881
[Seed=23] Epoch 31/40 => train_loss=0.2889, val_loss=0.2883
[Seed=23] Epoch 32/40 => train_loss=0.2880, val_loss=0.2882
[Seed=23] Epoch 33/40 => train_loss=0.2885, val_loss=0.2882
[Seed=23] Epoch 34/40 => train_loss=0.2877, val_loss=0.2883
[Seed=23] Epoch 35/40 => train_loss=0.2883, val_loss=0.2887
[Seed=23] Epoch 36/40 => train_loss=0.2880, val_loss=0.2885
[Seed=23] Epoch 37/40 => train_loss=0.2874, val_loss=0.2883
[Seed=23] Epoch 38/40 => train_loss=0.2874, val_loss=0.2886
[Seed=23] Epoch 39/40 => train_loss=0.2871, val_loss=0.2888
[Seed=23] Epoch 40/40 => train_loss=0.2876, val_loss=0.2885
[Seed=245] Epoch 1/40 => train_loss=4.4195, val_loss=4.1874
[Seed=245] Epoch 2/40 => train_loss=4.0959, val_loss=3.8377
[Seed=245] Epoch 3/40 => train_loss=1.8739, val_loss=0.3825
[Seed=245] Epoch 4/40 => train_loss=0.4460, val_loss=0.2951
[Seed=245] Epoch 5/40 => train_loss=0.3395, val_loss=0.2898
[Seed=245] Epoch 6/40 => train_loss=0.3156, val_loss=0.2891
[Seed=245] Epoch 7/40 => train_loss=0.3060, val_loss=0.2884
[Seed=245] Epoch 8/40 => train_loss=0.3023, val_loss=0.2884
[Seed=245] Epoch 9/40 => train_loss=0.2999, val_loss=0.2883
[Seed=245] Epoch 10/40 => train_loss=0.2964, val_loss=0.2883
[Seed=245] Epoch 11/40 => train_loss=0.2957, val_loss=0.2883
[Seed=245] Epoch 12/40 => train_loss=0.2932, val_loss=0.2882
[Seed=245] Epoch 13/40 => train_loss=0.2940, val_loss=0.2883
[Seed=245] Epoch 14/40 => train_loss=0.2922, val_loss=0.2883
[Seed=245] Epoch 15/40 => train_loss=0.2910, val_loss=0.2881
[Seed=245] Epoch 16/40 => train_loss=0.2930, val_loss=0.2881
[Seed=245] Epoch 17/40 => train_loss=0.2892, val_loss=0.2884
[Seed=245] Epoch 18/40 => train_loss=0.2907, val_loss=0.2881
[Seed=245] Epoch 19/40 => train_loss=0.2903, val_loss=0.2881
[Seed=245] Epoch 20/40 => train_loss=0.2903, val_loss=0.2882
[Seed=245] Epoch 21/40 => train_loss=0.2891, val_loss=0.2881
[Seed=245] Epoch 22/40 => train_loss=0.2887, val_loss=0.2880
[Seed=245] Epoch 23/40 => train_loss=0.2901, val_loss=0.2880
[Seed=245] Epoch 24/40 => train_loss=0.2882, val_loss=0.2880
[Seed=245] Epoch 25/40 => train_loss=0.2899, val_loss=0.2880
[Seed=245] Epoch 26/40 => train_loss=0.2886, val_loss=0.2881
[Seed=245] Epoch 27/40 => train_loss=0.2886, val_loss=0.2880
[Seed=245] Epoch 28/40 => train_loss=0.2890, val_loss=0.2880
[Seed=245] Epoch 29/40 => train_loss=0.2883, val_loss=0.2881
[Seed=245] Epoch 30/40 => train_loss=0.2885, val_loss=0.2880
[Seed=245] Epoch 31/40 => train_loss=0.2882, val_loss=0.2881
[Seed=245] Epoch 32/40 => train_loss=0.2878, val_loss=0.2881
[Seed=245] Epoch 33/40 => train_loss=0.2896, val_loss=0.2881
[Seed=245] Epoch 34/40 => train_loss=0.2878, val_loss=0.2882
[Seed=245] Epoch 35/40 => train_loss=0.2877, val_loss=0.2883
[Seed=245] Epoch 36/40 => train_loss=0.2876, val_loss=0.2881
[Seed=245] Epoch 37/40 => train_loss=0.2881, val_loss=0.2882
[Seed=245] Epoch 38/40 => train_loss=0.2875, val_loss=0.2883
[Seed=245] Epoch 39/40 => train_loss=0.2872, val_loss=0.2884
[Seed=245] Epoch 40/40 => train_loss=0.2876, val_loss=0.2889
[Seed=254] Epoch 1/40 => train_loss=4.4138, val_loss=4.1654
[Seed=254] Epoch 2/40 => train_loss=4.0607, val_loss=3.6598
[Seed=254] Epoch 3/40 => train_loss=1.5029, val_loss=0.4028
[Seed=254] Epoch 4/40 => train_loss=0.5108, val_loss=0.3118
[Seed=254] Epoch 5/40 => train_loss=0.3744, val_loss=0.2920
[Seed=254] Epoch 6/40 => train_loss=0.3315, val_loss=0.2904
[Seed=254] Epoch 7/40 => train_loss=0.3174, val_loss=0.2892
[Seed=254] Epoch 8/40 => train_loss=0.3071, val_loss=0.2885
[Seed=254] Epoch 9/40 => train_loss=0.3028, val_loss=0.2886
[Seed=254] Epoch 10/40 => train_loss=0.2995, val_loss=0.2884
[Seed=254] Epoch 11/40 => train_loss=0.2962, val_loss=0.2884
[Seed=254] Epoch 12/40 => train_loss=0.2941, val_loss=0.2885
[Seed=254] Epoch 13/40 => train_loss=0.2945, val_loss=0.2884
[Seed=254] Epoch 14/40 => train_loss=0.2930, val_loss=0.2884
[Seed=254] Epoch 15/40 => train_loss=0.2927, val_loss=0.2883
[Seed=254] Epoch 16/40 => train_loss=0.2907, val_loss=0.2884
[Seed=254] Epoch 17/40 => train_loss=0.2907, val_loss=0.2883
[Seed=254] Epoch 18/40 => train_loss=0.2913, val_loss=0.2891
[Seed=254] Epoch 19/40 => train_loss=0.2903, val_loss=0.2881
[Seed=254] Epoch 20/40 => train_loss=0.2907, val_loss=0.2883
[Seed=254] Epoch 21/40 => train_loss=0.2891, val_loss=0.2882
[Seed=254] Epoch 22/40 => train_loss=0.2900, val_loss=0.2881
[Seed=254] Epoch 23/40 => train_loss=0.2908, val_loss=0.2882
[Seed=254] Epoch 24/40 => train_loss=0.2884, val_loss=0.2881
[Seed=254] Epoch 25/40 => train_loss=0.2881, val_loss=0.2881
[Seed=254] Epoch 26/40 => train_loss=0.2896, val_loss=0.2881
[Seed=254] Epoch 27/40 => train_loss=0.2890, val_loss=0.2881
[Seed=254] Epoch 28/40 => train_loss=0.2883, val_loss=0.2882
[Seed=254] Epoch 29/40 => train_loss=0.2889, val_loss=0.2881
[Seed=254] Epoch 30/40 => train_loss=0.2886, val_loss=0.2881
[Seed=254] Epoch 31/40 => train_loss=0.2890, val_loss=0.2881
[Seed=254] Epoch 32/40 => train_loss=0.2879, val_loss=0.2881
[Seed=254] Epoch 33/40 => train_loss=0.2879, val_loss=0.2881
[Seed=254] Epoch 34/40 => train_loss=0.2878, val_loss=0.2882
[Seed=254] Epoch 35/40 => train_loss=0.2895, val_loss=0.2882
[Seed=254] Epoch 36/40 => train_loss=0.2879, val_loss=0.2882
[Seed=254] Epoch 37/40 => train_loss=0.2877, val_loss=0.2884
[Seed=254] Epoch 38/40 => train_loss=0.2876, val_loss=0.2884
[Seed=254] Epoch 39/40 => train_loss=0.2886, val_loss=0.2883
[Seed=254] Epoch 40/40 => train_loss=0.2876, val_loss=0.2883
[Seed=231] Epoch 1/40 => train_loss=4.4220, val_loss=4.1981
[Seed=231] Epoch 2/40 => train_loss=4.0875, val_loss=3.6490
[Seed=231] Epoch 3/40 => train_loss=1.4358, val_loss=0.3965
[Seed=231] Epoch 4/40 => train_loss=0.5120, val_loss=0.3227
[Seed=231] Epoch 5/40 => train_loss=0.3920, val_loss=0.2949
[Seed=231] Epoch 6/40 => train_loss=0.3408, val_loss=0.2906
[Seed=231] Epoch 7/40 => train_loss=0.3229, val_loss=0.2893
[Seed=231] Epoch 8/40 => train_loss=0.3129, val_loss=0.2887
[Seed=231] Epoch 9/40 => train_loss=0.3065, val_loss=0.2884
[Seed=231] Epoch 10/40 => train_loss=0.3048, val_loss=0.2886
[Seed=231] Epoch 11/40 => train_loss=0.3148, val_loss=0.2884
[Seed=231] Epoch 12/40 => train_loss=0.2924, val_loss=0.2882
[Seed=231] Epoch 13/40 => train_loss=0.2929, val_loss=0.2880
[Seed=231] Epoch 14/40 => train_loss=0.2950, val_loss=0.2882
[Seed=231] Epoch 15/40 => train_loss=0.2948, val_loss=0.2881
[Seed=231] Epoch 16/40 => train_loss=0.2973, val_loss=0.2890
[Seed=231] Epoch 17/40 => train_loss=0.2934, val_loss=0.2881
[Seed=231] Epoch 18/40 => train_loss=0.2935, val_loss=0.2882
[Seed=231] Epoch 19/40 => train_loss=0.2935, val_loss=0.2883
[Seed=231] Epoch 20/40 => train_loss=0.2923, val_loss=0.2880
[Seed=231] Epoch 21/40 => train_loss=0.2918, val_loss=0.2880
[Seed=231] Epoch 22/40 => train_loss=0.2911, val_loss=0.2880
[Seed=231] Epoch 23/40 => train_loss=0.2904, val_loss=0.2882
[Seed=231] Epoch 24/40 => train_loss=0.2910, val_loss=0.2880
[Seed=231] Epoch 25/40 => train_loss=0.2904, val_loss=0.2881
[Seed=231] Epoch 26/40 => train_loss=0.3030, val_loss=0.2880
[Seed=231] Epoch 27/40 => train_loss=0.2884, val_loss=0.2880
[Seed=231] Epoch 28/40 => train_loss=0.2884, val_loss=0.2880
[Seed=231] Epoch 29/40 => train_loss=0.2882, val_loss=0.2880
[Seed=231] Epoch 30/40 => train_loss=0.2885, val_loss=0.2881
[Seed=231] Epoch 31/40 => train_loss=0.2897, val_loss=0.2881
[Seed=231] Epoch 32/40 => train_loss=0.2885, val_loss=0.2879
[Seed=231] Epoch 33/40 => train_loss=0.2889, val_loss=0.2883
[Seed=231] Epoch 34/40 => train_loss=0.2892, val_loss=0.2882
[Seed=231] Epoch 35/40 => train_loss=0.2881, val_loss=0.2880
[Seed=231] Epoch 36/40 => train_loss=0.2888, val_loss=0.2880
[Seed=231] Epoch 37/40 => train_loss=0.2878, val_loss=0.2881
[Seed=231] Epoch 38/40 => train_loss=0.2885, val_loss=0.2882
[Seed=231] Epoch 39/40 => train_loss=0.2885, val_loss=0.2881
[Seed=231] Epoch 40/40 => train_loss=0.2876, val_loss=0.2882
[Seed=101] Epoch 1/40 => train_loss=4.4126, val_loss=4.1905
[Seed=101] Epoch 2/40 => train_loss=4.0580, val_loss=3.6196
[Seed=101] Epoch 3/40 => train_loss=1.1542, val_loss=0.3166
[Seed=101] Epoch 4/40 => train_loss=0.3168, val_loss=0.2927
[Seed=101] Epoch 5/40 => train_loss=0.3006, val_loss=0.2908
[Seed=101] Epoch 6/40 => train_loss=0.2960, val_loss=0.2903
[Seed=101] Epoch 7/40 => train_loss=0.2933, val_loss=0.2897
[Seed=101] Epoch 8/40 => train_loss=0.2920, val_loss=0.2895
[Seed=101] Epoch 9/40 => train_loss=0.2908, val_loss=0.2895
[Seed=101] Epoch 10/40 => train_loss=0.2901, val_loss=0.2892
[Seed=101] Epoch 11/40 => train_loss=0.2908, val_loss=0.2886
[Seed=101] Epoch 12/40 => train_loss=0.2891, val_loss=0.2885
[Seed=101] Epoch 13/40 => train_loss=0.2888, val_loss=0.2886
[Seed=101] Epoch 14/40 => train_loss=0.2887, val_loss=0.2885
[Seed=101] Epoch 15/40 => train_loss=0.2885, val_loss=0.2883
[Seed=101] Epoch 16/40 => train_loss=0.2895, val_loss=0.2885
[Seed=101] Epoch 17/40 => train_loss=0.2884, val_loss=0.2883
[Seed=101] Epoch 18/40 => train_loss=0.2889, val_loss=0.2888
[Seed=101] Epoch 19/40 => train_loss=0.2885, val_loss=0.2883
[Seed=101] Epoch 20/40 => train_loss=0.2882, val_loss=0.2882
[Seed=101] Epoch 21/40 => train_loss=0.2882, val_loss=0.2884
[Seed=101] Epoch 22/40 => train_loss=0.2890, val_loss=0.2884
[Seed=101] Epoch 23/40 => train_loss=0.2882, val_loss=0.2886
[Seed=101] Epoch 24/40 => train_loss=0.2881, val_loss=0.2884
[Seed=101] Epoch 25/40 => train_loss=0.2881, val_loss=0.2885
[Seed=101] Epoch 26/40 => train_loss=0.2880, val_loss=0.2884
[Seed=101] Epoch 27/40 => train_loss=0.2880, val_loss=0.2884
[Seed=101] Epoch 28/40 => train_loss=0.2888, val_loss=0.2884
[Seed=101] Epoch 29/40 => train_loss=0.2879, val_loss=0.2884
[Seed=101] Epoch 30/40 => train_loss=0.2878, val_loss=0.2885
[Seed=101] Epoch 31/40 => train_loss=0.2885, val_loss=0.2885
[Seed=101] Epoch 32/40 => train_loss=0.2878, val_loss=0.2885
[Seed=101] Epoch 33/40 => train_loss=0.2880, val_loss=0.2884
[Seed=101] Epoch 34/40 => train_loss=0.2877, val_loss=0.2885
[Seed=101] Epoch 35/40 => train_loss=0.2877, val_loss=0.2885
[Seed=101] Epoch 36/40 => train_loss=0.2884, val_loss=0.2886
[Seed=101] Epoch 37/40 => train_loss=0.2876, val_loss=0.2885
[Seed=101] Epoch 38/40 => train_loss=0.2875, val_loss=0.2886
[Seed=101] Epoch 39/40 => train_loss=0.2875, val_loss=0.2887
[Seed=101] Epoch 40/40 => train_loss=0.2879, val_loss=0.2887
[Seed=23] Epoch 1/40 => train_loss=4.4118, val_loss=4.1815
[Seed=23] Epoch 2/40 => train_loss=4.0689, val_loss=3.7441
[Seed=23] Epoch 3/40 => train_loss=1.4106, val_loss=0.3019
[Seed=23] Epoch 4/40 => train_loss=0.3084, val_loss=0.2919
[Seed=23] Epoch 5/40 => train_loss=0.2991, val_loss=0.2907
[Seed=23] Epoch 6/40 => train_loss=0.2952, val_loss=0.2899
[Seed=23] Epoch 7/40 => train_loss=0.2930, val_loss=0.2895
[Seed=23] Epoch 8/40 => train_loss=0.2916, val_loss=0.2890
[Seed=23] Epoch 9/40 => train_loss=0.2905, val_loss=0.2886
[Seed=23] Epoch 10/40 => train_loss=0.2900, val_loss=0.2887
[Seed=23] Epoch 11/40 => train_loss=0.2895, val_loss=0.2886
[Seed=23] Epoch 12/40 => train_loss=0.2891, val_loss=0.2884
[Seed=23] Epoch 13/40 => train_loss=0.2892, val_loss=0.2883
[Seed=23] Epoch 14/40 => train_loss=0.2889, val_loss=0.2883
[Seed=23] Epoch 15/40 => train_loss=0.2887, val_loss=0.2885
[Seed=23] Epoch 16/40 => train_loss=0.2886, val_loss=0.2884
[Seed=23] Epoch 17/40 => train_loss=0.2886, val_loss=0.2884
[Seed=23] Epoch 18/40 => train_loss=0.2887, val_loss=0.2885
[Seed=23] Epoch 19/40 => train_loss=0.2884, val_loss=0.2884
[Seed=23] Epoch 20/40 => train_loss=0.2885, val_loss=0.2888
[Seed=23] Epoch 21/40 => train_loss=0.2883, val_loss=0.2890
[Seed=23] Epoch 22/40 => train_loss=0.2884, val_loss=0.2886
[Seed=23] Epoch 23/40 => train_loss=0.2880, val_loss=0.2886
[Seed=23] Epoch 24/40 => train_loss=0.2881, val_loss=0.2883
[Seed=23] Epoch 25/40 => train_loss=0.2881, val_loss=0.2885
[Seed=23] Epoch 26/40 => train_loss=0.2880, val_loss=0.2885
[Seed=23] Epoch 27/40 => train_loss=0.2880, val_loss=0.2887
[Seed=23] Epoch 28/40 => train_loss=0.2879, val_loss=0.2887
[Seed=23] Epoch 29/40 => train_loss=0.2880, val_loss=0.2885
[Seed=23] Epoch 30/40 => train_loss=0.2877, val_loss=0.2886
[Seed=23] Epoch 31/40 => train_loss=0.2876, val_loss=0.2887
[Seed=23] Epoch 32/40 => train_loss=0.2876, val_loss=0.2887
[Seed=23] Epoch 33/40 => train_loss=0.2876, val_loss=0.2886
[Seed=23] Epoch 34/40 => train_loss=0.2874, val_loss=0.2889
[Seed=23] Epoch 35/40 => train_loss=0.2874, val_loss=0.2887
[Seed=23] Epoch 36/40 => train_loss=0.2874, val_loss=0.2887
[Seed=23] Epoch 37/40 => train_loss=0.2872, val_loss=0.2886
[Seed=23] Epoch 38/40 => train_loss=0.2871, val_loss=0.2889
[Seed=23] Epoch 39/40 => train_loss=0.2872, val_loss=0.2886
[Seed=23] Epoch 40/40 => train_loss=0.2869, val_loss=0.2888
[Seed=245] Epoch 1/40 => train_loss=4.4167, val_loss=4.1943
[Seed=245] Epoch 2/40 => train_loss=4.1199, val_loss=3.9647
[Seed=245] Epoch 3/40 => train_loss=2.7248, val_loss=0.4208
[Seed=245] Epoch 4/40 => train_loss=0.3383, val_loss=0.2926
[Seed=245] Epoch 5/40 => train_loss=0.2978, val_loss=0.2906
[Seed=245] Epoch 6/40 => train_loss=0.2934, val_loss=0.2895
[Seed=245] Epoch 7/40 => train_loss=0.2912, val_loss=0.2891
[Seed=245] Epoch 8/40 => train_loss=0.2902, val_loss=0.2887
[Seed=245] Epoch 9/40 => train_loss=0.2896, val_loss=0.2888
[Seed=245] Epoch 10/40 => train_loss=0.2890, val_loss=0.2887
[Seed=245] Epoch 11/40 => train_loss=0.2887, val_loss=0.2885
[Seed=245] Epoch 12/40 => train_loss=0.2885, val_loss=0.2884
[Seed=245] Epoch 13/40 => train_loss=0.2883, val_loss=0.2885
[Seed=245] Epoch 14/40 => train_loss=0.2881, val_loss=0.2884
[Seed=245] Epoch 15/40 => train_loss=0.2881, val_loss=0.2885
[Seed=245] Epoch 16/40 => train_loss=0.2879, val_loss=0.2885
[Seed=245] Epoch 17/40 => train_loss=0.2878, val_loss=0.2885
[Seed=245] Epoch 18/40 => train_loss=0.2876, val_loss=0.2886
[Seed=245] Epoch 19/40 => train_loss=0.2876, val_loss=0.2886
[Seed=245] Epoch 20/40 => train_loss=0.2874, val_loss=0.2888
[Seed=245] Epoch 21/40 => train_loss=0.2873, val_loss=0.2887
[Seed=245] Epoch 22/40 => train_loss=0.2872, val_loss=0.2889
[Seed=245] Epoch 23/40 => train_loss=0.2871, val_loss=0.2890
[Seed=245] Epoch 24/40 => train_loss=0.2870, val_loss=0.2889
[Seed=245] Epoch 25/40 => train_loss=0.2869, val_loss=0.2889
[Seed=245] Epoch 26/40 => train_loss=0.2867, val_loss=0.2890
[Seed=245] Epoch 27/40 => train_loss=0.2866, val_loss=0.2892
[Seed=245] Epoch 28/40 => train_loss=0.2865, val_loss=0.2895
[Seed=245] Epoch 29/40 => train_loss=0.2863, val_loss=0.2892
[Seed=245] Epoch 30/40 => train_loss=0.2862, val_loss=0.2897
[Seed=245] Epoch 31/40 => train_loss=0.2862, val_loss=0.2893
[Seed=245] Epoch 32/40 => train_loss=0.2861, val_loss=0.2895
[Seed=245] Epoch 33/40 => train_loss=0.2861, val_loss=0.2894
[Seed=245] Epoch 34/40 => train_loss=0.2859, val_loss=0.2903
[Seed=245] Epoch 35/40 => train_loss=0.2858, val_loss=0.2900
[Seed=245] Epoch 36/40 => train_loss=0.2857, val_loss=0.2898
[Seed=245] Epoch 37/40 => train_loss=0.2856, val_loss=0.2900
[Seed=245] Epoch 38/40 => train_loss=0.2855, val_loss=0.2900
[Seed=245] Epoch 39/40 => train_loss=0.2855, val_loss=0.2906
[Seed=245] Epoch 40/40 => train_loss=0.2853, val_loss=0.2901
[Seed=254] Epoch 1/40 => train_loss=4.4087, val_loss=4.1646
[Seed=254] Epoch 2/40 => train_loss=4.0261, val_loss=3.5419
[Seed=254] Epoch 3/40 => train_loss=1.7471, val_loss=0.4657
[Seed=254] Epoch 4/40 => train_loss=0.7260, val_loss=0.3619
[Seed=254] Epoch 5/40 => train_loss=0.4741, val_loss=0.3045
[Seed=254] Epoch 6/40 => train_loss=0.4297, val_loss=0.2983
[Seed=254] Epoch 7/40 => train_loss=0.3724, val_loss=0.2910
[Seed=254] Epoch 8/40 => train_loss=0.3353, val_loss=0.2906
[Seed=254] Epoch 9/40 => train_loss=0.3183, val_loss=0.2895
[Seed=254] Epoch 10/40 => train_loss=0.3113, val_loss=0.2895
[Seed=254] Epoch 11/40 => train_loss=0.3037, val_loss=0.2896
[Seed=254] Epoch 12/40 => train_loss=0.3005, val_loss=0.2893
[Seed=254] Epoch 13/40 => train_loss=0.2966, val_loss=0.2891
[Seed=254] Epoch 14/40 => train_loss=0.2947, val_loss=0.2889
[Seed=254] Epoch 15/40 => train_loss=0.2928, val_loss=0.2887
[Seed=254] Epoch 16/40 => train_loss=0.2925, val_loss=0.2894
[Seed=254] Epoch 17/40 => train_loss=0.2936, val_loss=0.2883
[Seed=254] Epoch 18/40 => train_loss=0.2900, val_loss=0.2885
[Seed=254] Epoch 19/40 => train_loss=0.2895, val_loss=0.2888
[Seed=254] Epoch 20/40 => train_loss=0.2896, val_loss=0.2889
[Seed=254] Epoch 21/40 => train_loss=0.2890, val_loss=0.2884
[Seed=254] Epoch 22/40 => train_loss=0.2902, val_loss=0.2883
[Seed=254] Epoch 23/40 => train_loss=0.2890, val_loss=0.2882
[Seed=254] Epoch 24/40 => train_loss=0.2886, val_loss=0.2883
[Seed=254] Epoch 25/40 => train_loss=0.2891, val_loss=0.2882
[Seed=254] Epoch 26/40 => train_loss=0.2886, val_loss=0.2883
[Seed=254] Epoch 27/40 => train_loss=0.2885, val_loss=0.2883
[Seed=254] Epoch 28/40 => train_loss=0.2890, val_loss=0.2881
[Seed=254] Epoch 29/40 => train_loss=0.2885, val_loss=0.2883
[Seed=254] Epoch 30/40 => train_loss=0.2884, val_loss=0.2883
[Seed=254] Epoch 31/40 => train_loss=0.2883, val_loss=0.2882
[Seed=254] Epoch 32/40 => train_loss=0.2882, val_loss=0.2882
[Seed=254] Epoch 33/40 => train_loss=0.2888, val_loss=0.2883
[Seed=254] Epoch 34/40 => train_loss=0.2883, val_loss=0.2883
[Seed=254] Epoch 35/40 => train_loss=0.2881, val_loss=0.2882
[Seed=254] Epoch 36/40 => train_loss=0.2881, val_loss=0.2881
[Seed=254] Epoch 37/40 => train_loss=0.2882, val_loss=0.2881
[Seed=254] Epoch 38/40 => train_loss=0.2881, val_loss=0.2881
[Seed=254] Epoch 39/40 => train_loss=0.2885, val_loss=0.2883
[Seed=254] Epoch 40/40 => train_loss=0.2880, val_loss=0.2884
[Seed=231] Epoch 1/40 => train_loss=4.4135, val_loss=4.1919
[Seed=231] Epoch 2/40 => train_loss=4.0918, val_loss=3.8366
[Seed=231] Epoch 3/40 => train_loss=1.4163, val_loss=0.3125
[Seed=231] Epoch 4/40 => train_loss=0.3129, val_loss=0.2930
[Seed=231] Epoch 5/40 => train_loss=0.2994, val_loss=0.2904
[Seed=231] Epoch 6/40 => train_loss=0.2944, val_loss=0.2894
[Seed=231] Epoch 7/40 => train_loss=0.2921, val_loss=0.2888
[Seed=231] Epoch 8/40 => train_loss=0.2906, val_loss=0.2886
[Seed=231] Epoch 9/40 => train_loss=0.2899, val_loss=0.2882
[Seed=231] Epoch 10/40 => train_loss=0.2894, val_loss=0.2884
[Seed=231] Epoch 11/40 => train_loss=0.2891, val_loss=0.2881
[Seed=231] Epoch 12/40 => train_loss=0.2889, val_loss=0.2883
[Seed=231] Epoch 13/40 => train_loss=0.2887, val_loss=0.2881
[Seed=231] Epoch 14/40 => train_loss=0.2886, val_loss=0.2881
[Seed=231] Epoch 15/40 => train_loss=0.2884, val_loss=0.2882
[Seed=231] Epoch 16/40 => train_loss=0.2883, val_loss=0.2882
[Seed=231] Epoch 17/40 => train_loss=0.2882, val_loss=0.2884
[Seed=231] Epoch 18/40 => train_loss=0.2884, val_loss=0.2883
[Seed=231] Epoch 19/40 => train_loss=0.2881, val_loss=0.2882
[Seed=231] Epoch 20/40 => train_loss=0.2881, val_loss=0.2882
[Seed=231] Epoch 21/40 => train_loss=0.2880, val_loss=0.2882
[Seed=231] Epoch 22/40 => train_loss=0.2879, val_loss=0.2883
[Seed=231] Epoch 23/40 => train_loss=0.2878, val_loss=0.2892
[Seed=231] Epoch 24/40 => train_loss=0.2879, val_loss=0.2884
[Seed=231] Epoch 25/40 => train_loss=0.2878, val_loss=0.2883
[Seed=231] Epoch 26/40 => train_loss=0.2877, val_loss=0.2885
[Seed=231] Epoch 27/40 => train_loss=0.2875, val_loss=0.2885
[Seed=231] Epoch 28/40 => train_loss=0.2875, val_loss=0.2886
[Seed=231] Epoch 29/40 => train_loss=0.2874, val_loss=0.2884
[Seed=231] Epoch 30/40 => train_loss=0.2873, val_loss=0.2885
[Seed=231] Epoch 31/40 => train_loss=0.2872, val_loss=0.2888
[Seed=231] Epoch 32/40 => train_loss=0.2870, val_loss=0.2890
[Seed=231] Epoch 33/40 => train_loss=0.2870, val_loss=0.2889
[Seed=231] Epoch 34/40 => train_loss=0.2868, val_loss=0.2891
[Seed=231] Epoch 35/40 => train_loss=0.2867, val_loss=0.2891
[Seed=231] Epoch 36/40 => train_loss=0.2867, val_loss=0.2891
[Seed=231] Epoch 37/40 => train_loss=0.2866, val_loss=0.2892
[Seed=231] Epoch 38/40 => train_loss=0.2864, val_loss=0.2894
[Seed=231] Epoch 39/40 => train_loss=0.2863, val_loss=0.2895
[Seed=231] Epoch 40/40 => train_loss=0.2862, val_loss=0.2897
Plots saved in: moe_transformer_experiment

=== Summary (Time, Mem, TestAcc) ===
('softmax', 'dense'): Time mean=370.50s [CI=(368.19,372.47)], Mem mean=46.06MB [CI=(46.06,46.06)], TestAcc=93.81% [CI=(93.81,93.82)]
('softmax', 'sparse'): Time mean=164.95s [CI=(162.38,167.52)], Mem mean=29.39MB [CI=(29.39,29.39)], TestAcc=93.81% [CI=(93.81,93.82)]

=== Explanation: Sparse vs. Dense MoE ===
In a Dense MoE, every expert's output is computed regardless of gating. This can be slower when many experts exist. In a Sparse MoE, the gating network produces a binary mask that 'skips' inactive experts via a custom block-sparse kernel. Hence, if many experts are zeroed out, we do less compute, reducing time & memory.
Gating methods (top-k, sparsemax, thresholded softmax/sigmoid, quadratic, pioneer) select only a subset of experts per sample. This leads to speedups on modern GPUs, because large parts of the weight matrix are never multiplied.
In contrast, a Dense MoE calculates outputs from all experts every time, increasing compute cost if many experts exist.
Done! Check the plots and summary in moe_transformer_experiment
